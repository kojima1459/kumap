# ひぐまっぷ調査レポート

## 概要
- **サイト名**: ひぐまっぷ
- **URL**: https://higumap.info/
- **運営**: ダッピスタジオ合同会社
- **対象地域**: 北海道全域（67市町村が参加）
- **データ公開**: 直近3ヶ月のヒグマ出没情報

## データ構造

### 表示形式
- Leaflet.js ベースの地図
- マーカーで出没位置を表示
- マーカーの色分け:
  - 赤: 目撃情報
  - オレンジ: 足跡・糞などの痕跡
  - グレー: 情報が古い（3ヶ月以上経過）

### 技術スタック
- フロントエンド: Leaflet.js
- 地図タイル: 国土地理院
- 認証: ログインシステムあり（市町村担当者用）

## スクレイピング可能性の評価

### ✅ 利点
1. **公開マップが存在**: https://higumap.info/recent で直近3ヶ月のデータが公開
2. **Leafletベース**: JavaScriptで動的に生成されるため、ブラウザ自動化で取得可能
3. **広域カバー**: 北海道全域67市町村のデータが一元化
4. **リアルタイム更新**: 市町村担当者が随時更新

### ⚠️ 課題
1. **動的コンテンツ**: マーカーデータがJavaScriptで動的に生成される
2. **APIの有無不明**: 公開APIがあるかどうか不明（要調査）
3. **データ詳細**: マーカーをクリックしないと詳細情報が取得できない可能性
4. **ログイン要否**: 全データにアクセスするにはログインが必要な可能性

## スクレイピング戦略

### 方法1: ブラウザ自動化（Playwright MCP）
- Playwright MCPを使用してブラウザを自動操作
- ページを読み込み、マーカーデータを抽出
- 各マーカーをクリックして詳細情報を取得
- **推定難易度**: 中（JavaScriptの解析が必要）

### 方法2: ネットワークリクエストの解析
- ブラウザの開発者ツールでネットワークリクエストを確認
- マーカーデータを取得するAPIエンドポイントを特定
- 直接APIを叩いてデータを取得
- **推定難易度**: 低（APIが公開されている場合）

### 方法3: 市町村個別ページからの取得
- 各市町村の個別ページ（例: https://higumap.info/eniwa）からデータを取得
- 67市町村分のスクレイピングが必要
- **推定難易度**: 高（市町村ごとに構造が異なる可能性）

## 次のステップ

1. ✅ ブラウザ開発者ツールでネットワークリクエストを確認
2. ✅ APIエンドポイントの特定
3. ⏳ テストスクレイピングの実装
4. ⏳ データ形式の確認（緯度経度、日時、詳細情報）
5. ⏳ 重複排除ロジックとの統合

## 法的・倫理的考慮事項

- **利用規約**: サイトに明示的な利用規約が見当たらない
- **robots.txt**: 確認が必要
- **データの帰属**: 各市町村が提供する公開情報
- **推奨アプローチ**: 
  1. 運営元（ダッピスタジオ合同会社）に問い合わせ
  2. APIの公式提供を依頼
  3. 許可が得られない場合は、個別市町村の公式サイトから取得

## 結論

ひぐまっぷは北海道のヒグマ出没情報を一元化した優れたシステムだが、スクレイピングの実装には技術的・法的な課題がある。まずはAPIの有無を確認し、公式な方法でのデータ取得を検討すべき。
